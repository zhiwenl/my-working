{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data, depth=0, lchild=None, rchild=None):\n",
    "        self.data = data\n",
    "        self.depth = depth\n",
    "        self.lchild = lchild\n",
    "        self.rchild = rchild\n",
    "\n",
    "\n",
    "class KdTree:\n",
    "    def __init__(self):\n",
    "        self.KdTree = None\n",
    "        self.n = 0\n",
    "        self.nearest = None\n",
    "\n",
    "    def create(self, dataSet, depth=0):\n",
    "        if len(dataSet) > 0:\n",
    "            m, n = np.shape(dataSet)\n",
    "            self.n = n - 1\n",
    "            axis = depth % self.n\n",
    "            mid = int(m / 2)\n",
    "            dataSetcopy = sorted(dataSet, key=lambda x: x[axis])\n",
    "            node = Node(dataSetcopy[mid], depth)\n",
    "            if depth == 0:\n",
    "                self.KdTree = node\n",
    "            node.lchild = self.create(dataSetcopy[:mid], depth+1)\n",
    "            node.rchild = self.create(dataSetcopy[mid+1:], depth+1)\n",
    "            return node\n",
    "        return None\n",
    "\n",
    "    def preOrder(self, node):\n",
    "        if node is not None:\n",
    "            print(node.depth, node.data)\n",
    "            self.preOrder(node.lchild)\n",
    "            self.preOrder(node.rchild)\n",
    "\n",
    "    def search(self, x, count=1):\n",
    "        nearest = []\n",
    "        for i in range(count):\n",
    "            nearest.append([-1, None])\n",
    "        self.nearest = np.array(nearest)\n",
    "\n",
    "        def recurve(node):\n",
    "            if node is not None:\n",
    "                axis = node.depth % self.n\n",
    "                daxis = x[axis] - node.data[axis]\n",
    "                if daxis < 0:\n",
    "                    recurve(node.lchild)\n",
    "                else:\n",
    "                    recurve(node.rchild)\n",
    "\n",
    "                dist = sqrt(sum((p1 - p2) ** 2 for p1, p2 in zip(x, node.data)))\n",
    "                for i, d in enumerate(self.nearest):\n",
    "                    if d[0] < 0 or dist < d[0]:\n",
    "                        self.nearest = np.insert(self.nearest, i, [dist, node], axis=0)\n",
    "                        self.nearest = self.nearest[:-1]\n",
    "                        break\n",
    "\n",
    "                n = list(self.nearest[:, 0]).count(-1)\n",
    "                if self.nearest[-n-1, 0] > abs(daxis):\n",
    "                    if daxis < 0:\n",
    "                        recurve(node.rchild)\n",
    "                    else:\n",
    "                        recurve(node.lchild)\n",
    "\n",
    "        recurve(self.KdTree)\n",
    "\n",
    "        knn = self.nearest[:, 1]\n",
    "        return self.nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mpl_toolkits.axisartist.axislines as axislines\n",
    "import matplotlib as mpl\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler=preprocessing.MinMaxScaler()\n",
    "mpl.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(n_splits=10)\n",
    "\n",
    "def DDF(df):    \n",
    "    data2=np.array(df.drop('label',axis=1))\n",
    "    dmax=[]\n",
    "    for m in range(0,len(data2)-1):\n",
    "        for n in range(m+1,len(data2)):\n",
    "            dmax.append(sqrt(sum((data2[m] - data2[n]) ** 2 )))\n",
    "    a = np.percentile(dmax,10) #密度阈值(%10分位数)\n",
    "    k=7\n",
    "    x1 = np.array([[1,1],[1,-1]])\n",
    "    x2 = np.array([1,np.floor(k / 3)/ k])\n",
    "    (Ps_,Pd_)=np.linalg.solve(x1,x2)\n",
    "    H_ = -Ps_* np.log2(Ps_)-Pd_*np.log2(Pd_) #标签混合程度阈值\n",
    "    Fh,Re=[],[]\n",
    "    data=np.array(df)\n",
    "    for rate in [0.05,0.1,0.2,0.3,0.4]:\n",
    "        fh_temp,re_temp=[],[]\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            train=data[train_index]\n",
    "            test=data[test_index]\n",
    "            train=pd.DataFrame(train)\n",
    "            train.columns=df.columns\n",
    "            Dn= []\n",
    "            noise_set=pd.DataFrame(columns=train.columns)\n",
    "            label_set=list(set(train.label))\n",
    "            for r in label_set:    \n",
    "                noise_set=pd.concat([noise_set,train[train.label==r].sample(frac=rate,replace=False)]) #选取噪声比例 \n",
    "            train_cut=train[~train.index.isin(noise_set.index)]\n",
    "            no=[]\n",
    "            for j in range(0,len(noise_set)):\n",
    "                no.append(np.random.choice(label_set))\n",
    "            noise_set['label2']=no\n",
    "            noise_set.reset_index(drop=True,inplace=True)\n",
    "            for l in range(0,len(noise_set)):  #随机替换标签\n",
    "                label_set=list(set(train.label))\n",
    "                if (noise_set.loc[l,'label']==noise_set.loc[l,'label2']):\n",
    "                    label_set.remove(noise_set.loc[l,'label'])\n",
    "                    noise_set.loc[l,'label2']=np.random.choice(label_set) #从剩余列表中随机选择\n",
    "            noise=noise_set.drop(['label'],axis=1)\n",
    "            noise.rename(columns={'label2':'label'}, inplace = True)\n",
    "            train= pd.concat([train_cut,noise],axis=0,ignore_index=True) #含噪声数据集\n",
    "            train_set=np.array(train)\n",
    "            kdt = KdTree()\n",
    "            kdt.create(train_set)  \n",
    "            for x in train_set:\n",
    "                near= kdt.search(x[:-1], k+1)  # 设置临近点的个数\n",
    "                density = 0\n",
    "                t,hon,hen,dist_hon,dist_hen,DoD,DRL= 0,0,0,0,0,0,0\n",
    "                for i in range(1,k+1):\n",
    "                    if x[-1] == near[i][1].data[-1]:\n",
    "                        t += 1\n",
    "                        hon += 1\n",
    "                        dist_hon += near[i][0]\n",
    "                    else:\n",
    "                        hen += 1\n",
    "                        dist_hen += near[i][0]\n",
    "\n",
    "                    density += near[i][0]  #密度   \n",
    "                DoD = abs(dist_hen - dist_hon) #相异性差值(要加绝对值)\n",
    "                DRL = (hen - hon)  #标签异同差\n",
    "                Ps = t / k\n",
    "                Pd = 1 - Ps\n",
    "                if (Ps==1)|(Ps==0):\n",
    "                    H = 0\n",
    "                else:\n",
    "                    H = -Ps* np.log2(Ps)-Pd*np.log2(Pd)\n",
    "              #  print('densit:',density)\n",
    "                if (density <= a): #高密度区域\n",
    "                   # print('高密度区')\n",
    "                    if DRL > 0:\n",
    "                        Dn.append(list(x))\n",
    "                       # print('高密度区噪声')\n",
    "                else:\n",
    "                    if (H < H_): #低密度单一标签区域\n",
    "                        if DRL > 0:\n",
    "                            Dn.append(list(x))\n",
    "                           # print('低密度单一标签噪声'\n",
    "                    else: #低密度混合标签区  \n",
    "                      #  print('DoD',DoD)\n",
    "                        DoD_ = density / k\n",
    "                       # print('DoD阈值:',DoD_)\n",
    "                        if DoD >= DoD_:\n",
    "                            hon_sa,hen_sa,DRL_sa=0,0,0\n",
    "                            near2=[]\n",
    "                            for i in range(1,k+1):\n",
    "                                if near[i][0] <= a: #该点到待测点距离小于a\n",
    "                                    near2.append(near[i])\n",
    "                            for n2 in near2:\n",
    "                                if (x[-1]==n2[1].data[-1]):\n",
    "                                    hon_sa +=1\n",
    "                                else:\n",
    "                                    hen_sa +=1\n",
    "                            DRL_sa = hen_sa - hon_sa\n",
    "                            if DRL_sa > 0:        \n",
    "                                Dn.append(list(x))\n",
    "                               # print('混合标签区噪声')\n",
    "                            elif DRL_sa ==0 and len(near2)<k:\n",
    "                                if x[-1] != near[len(near2)+1][1].data[-1]:\n",
    "                                    Dn.append(list(x))  \n",
    "\n",
    "            noise_list=noise.values.tolist()\n",
    "            TP = 0\n",
    "            for c in Dn:\n",
    "                if c in noise_list:\n",
    "                    TP += 1\n",
    "            FP = len(Dn) - TP\n",
    "            FN = len(noise_list) - TP\n",
    "            precision= TP / (TP + FP)\n",
    "            recall= TP / (TP + FN)\n",
    "            fh= 1.25* (precision * recall) / (0.25*precision + recall) #F0.5值\n",
    "            re = len(Dn) / len(train_set)#移除率\n",
    "            fh_temp.append(fh)\n",
    "            re_temp.append(re)\n",
    "        Fh.append(np.mean(fh_temp))\n",
    "        Re.append(np.mean(re_temp))\n",
    "    return Fh,Re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RDS(df):    \n",
    "    k=7\n",
    "    x1 = np.array([[1,1],[1,-1]])\n",
    "    x2 = np.array([1,np.floor(k / 3)/ k])\n",
    "    (Ps_,Pd_)=np.linalg.solve(x1,x2)\n",
    "    H_ = -Ps_* np.log2(Ps_)-Pd_*np.log2(Pd_) #标签混合程度阈值\n",
    "    data2=np.array(df.drop('label',axis=1))\n",
    "    dmax=[]\n",
    "    for m in range(0,len(data2)-1):\n",
    "        for n in range(m+1,len(data2)):\n",
    "            dmax.append(sqrt(sum((data2[m] - data2[n]) ** 2 )))\n",
    "    a = np.percentile(dmax,10) #密度阈值(%10分位数)\n",
    "    Fh,Re=[],[]\n",
    "    data=np.array(df)\n",
    "    for rate in [0.05,0.1,0.2,0.3,0.4]:\n",
    "        fh_temp,re_temp=[],[]\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            train=data[train_index]\n",
    "            test=data[test_index]\n",
    "            train=pd.DataFrame(train)\n",
    "            train.columns=df.columns\n",
    "            Dn,Dn_filted= [],[]\n",
    "            noise_set=pd.DataFrame(columns=train.columns)\n",
    "            label_set=list(set(train.label))\n",
    "            for r in label_set:    \n",
    "                noise_set=pd.concat([noise_set,train[train.label==r].sample(frac=rate, replace=False)]) #选取噪声比例 \n",
    "            train_cut=train[~train.index.isin(noise_set.index)]\n",
    "            no=[]\n",
    "            for j in range(0,len(noise_set)):\n",
    "                no.append(np.random.choice(label_set))\n",
    "            noise_set['label2']=no\n",
    "            noise_set.reset_index(drop=True,inplace=True)\n",
    "            for l in range(0,len(noise_set)):  #随机替换标签\n",
    "                label_set=list(set(train.label))\n",
    "                if (noise_set.loc[l,'label']==noise_set.loc[l,'label2']):\n",
    "                    label_set.remove(noise_set.loc[l,'label'])\n",
    "                    noise_set.loc[l,'label2']=np.random.choice(label_set) #从剩余列表中随机选择\n",
    "            noise=noise_set.drop(['label'],axis=1)\n",
    "            noise.rename(columns={'label2':'label'}, inplace = True)\n",
    "            train= pd.concat([train_cut,noise],axis=0,ignore_index=True) #含噪声数据集\n",
    "            train_set=np.array(train)\n",
    "            kdt = KdTree()\n",
    "            kdt.create(train_set)  \n",
    "            for x in train_set:\n",
    "                near = kdt.search(x[:-1], k+1)  # 设置临近点的个数\n",
    "                density = 0\n",
    "                t,hon,hen,dist_hon,dist_hen,DoD,DRL= 0,0,0,0,0,0,0\n",
    "                for i in range(1,k+1):\n",
    "                    if x[-1] == near[i][1].data[-1]:\n",
    "                        t += 1\n",
    "                        hon += 1\n",
    "                        dist_hon += near[i][0]\n",
    "                    else:\n",
    "                        hen += 1\n",
    "                        dist_hen += near[i][0]\n",
    "                    density += near[i][0]  #密度   \n",
    "\n",
    "                DoD = abs(dist_hen - dist_hon) #相异性差值(要加绝对值)\n",
    "                DRL = (hen - hon)  #标签异同差   \n",
    "                Ps = t / k\n",
    "                Pd = 1 - Ps\n",
    "                if (Ps==1)|(Ps==0):\n",
    "                    H = 0\n",
    "                else:\n",
    "                    H = -Ps* np.log2(Ps)-Pd*np.log2(Pd)\n",
    "              #  print('densit:',density)\n",
    "                if (density <= a): #高密度区域\n",
    "                    if DRL > 0:\n",
    "                        Dn.append(list(x))\n",
    "                else:\n",
    "                    if (H < H_): #低密度单一标签区域\n",
    "                        if DRL > 0:\n",
    "                            Dn.append(list(x))\n",
    "                           # print('低密度单一标签噪声')\n",
    "                    else: #低密度混合标签区  \n",
    "                        DoD_ = density / k\n",
    "                        if DoD >= DoD_:\n",
    "                            train_hon,train_hen=[],[]\n",
    "                            for j in train_set:\n",
    "                                if j[-1]==x[-1]:\n",
    "                                    train_hon.append(j)\n",
    "                                else:\n",
    "                                    train_hen.append(j)\n",
    "                            kdt_hon=KdTree()\n",
    "                            kdt_hen=KdTree()\n",
    "                            kdt_hon.create(train_hon)\n",
    "                            kdt_hen.create(train_hen)\n",
    "                            near_hon=kdt_hon.search(x,k+1)\n",
    "                            near_hen=kdt_hen.search(x,k)\n",
    "                            density_hon,density_hen=0,0\n",
    "                            for hon in near_hon:\n",
    "                                density_hon += hon[0]\n",
    "                            for hen in near_hen:\n",
    "                                density_hen += hen[0]\n",
    "                            if density_hon / density_hen > 1:\n",
    "                                Dn.append(list(x))                 \n",
    "            noise_neighbor=[]                   \n",
    "            for e in Dn:  # 计算噪声分数\n",
    "                e=np.array(e)\n",
    "                near2=kdt.search(e[:-1],k+1) #k近邻\n",
    "                for i in range(1,k+1):\n",
    "                    noise_neighbor.append(list(near2[i][1].data))\n",
    "            for e in Dn:\n",
    "                e=np.array(e)\n",
    "                near3=kdt.search(e[:-1],k+1)\n",
    "                t_e=noise_neighbor.count(list(e)) #处于其它噪声近邻的次数\n",
    "                confidence_e= 1 / np.sqrt(1+t_e*t_e)#e的置信度\n",
    "                neighborhood_e=0\n",
    "                for i in range(1,k+1):\n",
    "                    t_ei=noise_neighbor.count(list(near3[i][1].data))\n",
    "                    confidence_ei=1 / np.sqrt(1+t_ei*t_ei) \n",
    "                    if near3[i][1].data[-1]==e[-1]:\n",
    "                        different_class=-1\n",
    "                    else:\n",
    "                        different_class=1\n",
    "                    if list(near3[i][1].data) in Dn: \n",
    "                        isnoise=1\n",
    "                    else:\n",
    "                        isnoise=-1\n",
    "                    near4=kdt.search(near3[i][1].data[:-1],k+1)\n",
    "                    n_ei=0\n",
    "                    for j in range(1,k+1):\n",
    "                        if list(near4[j][1].data) in Dn:\n",
    "                            n_ei += 1\n",
    "                    clean_ei= (k + isnoise*(n_ei- k)) / (2*k) #纯净度\n",
    "                    neighborhood_e += (clean_ei*confidence_ei*different_class)/k \n",
    "                NS_e=confidence_e * neighborhood_e #噪声得分\n",
    "                if NS_e > 0:\n",
    "                    Dn_filted.append(list(e))\n",
    "            noise_list=noise.values.tolist()\n",
    "            TP = 0\n",
    "            for c in Dn_filted:\n",
    "                if c in noise_list:\n",
    "                    TP += 1\n",
    "            FP = len(Dn_filted) - TP\n",
    "            FN = len(noise_list) - TP\n",
    "            precision= TP / (TP + FP)\n",
    "            recall= TP / (TP + FN)\n",
    "            fh= 1.25* (precision * recall) / (0.25*precision + recall) #F0.5值\n",
    "            re = len(Dn_filted) / len(train_set)#移除率\n",
    "            fh_temp.append(fh)\n",
    "            re_temp.append(re)\n",
    "        Fh.append(np.mean(fh_temp))\n",
    "        Re.append(np.mean(re_temp))\n",
    "    return Fh,Re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#iris数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris_sample= pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(iris_sample)))\n",
    "iris_sample['label'] = iris.target\n",
    "iris_DDF=DDF(iris_sample)\n",
    "iris_RDS=RDS(iris_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.6668976789800588,\n",
       "  0.8234629083627233,\n",
       "  0.8386866164195552,\n",
       "  0.8059574780381616,\n",
       "  0.7581342638295521],\n",
       " [0.07259259259259258,\n",
       "  0.12592592592592594,\n",
       "  0.24444444444444446,\n",
       "  0.3718518518518518,\n",
       "  0.5037037037037038])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_DDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.7309049477914736,\n",
       "  0.8900053077311642,\n",
       "  0.904610919061609,\n",
       "  0.863910499316056,\n",
       "  0.843531542330569],\n",
       " [0.06444444444444444,\n",
       "  0.1162962962962963,\n",
       "  0.22148148148148147,\n",
       "  0.30518518518518517,\n",
       "  0.3874074074074074])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_RDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#wine数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.datasets import load_wine\n",
    "wine= load_wine()\n",
    "wine_sample = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "wine_sample = pd.DataFrame(min_max_scaler.fit_transform(np.array(wine_sample)))\n",
    "wine_sample['label'] = wine.target\n",
    "wine_DDF=DDF(wine_sample)\n",
    "wine_RDS=RDS(wine_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.6142558164818439,\n",
       "  0.7270105720949049,\n",
       "  0.7825791973618494,\n",
       "  0.7644057808860316,\n",
       "  0.7607841236569132],\n",
       " [0.08550077639751552,\n",
       "  0.14355201863354036,\n",
       "  0.2627756211180124,\n",
       "  0.3776552795031056,\n",
       "  0.5000349378881987])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_DDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.7552050936484899,\n",
       "  0.8539624505928854,\n",
       "  0.9143110615970468,\n",
       "  0.8784243715745834,\n",
       "  0.8436790445195846],\n",
       " [0.07116459627329193,\n",
       "  0.11860636645962734,\n",
       "  0.21536878881987578,\n",
       "  0.29276009316770185,\n",
       "  0.39449534161490685])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_RDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#seeds数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seeds=pd.read_csv('C:/Users/liu/python works/mydata/seeds.csv',engine='python')\n",
    "seeds_sample=seeds.drop(['label'],axis=1)\n",
    "seeds_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(seeds_sample)))\n",
    "seeds_sample['label']=seeds.label\n",
    "seeds_DDF=DDF(seeds_sample)\n",
    "seeds_RDS=RDS(seeds_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5312525205585656,\n",
       "  0.6542400778265756,\n",
       "  0.751130297341153,\n",
       "  0.7235986005599813,\n",
       "  0.7419203151899387],\n",
       " [0.108994708994709,\n",
       "  0.15978835978835979,\n",
       "  0.27142857142857146,\n",
       "  0.3994708994708994,\n",
       "  0.5021164021164021])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seeds_DDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5989656647342412,\n",
       "  0.7390049631295309,\n",
       "  0.8369258745068956,\n",
       "  0.8568772071312475,\n",
       "  0.816129963962841],\n",
       " [0.08783068783068784,\n",
       "  0.13544973544973543,\n",
       "  0.2312169312169312,\n",
       "  0.3190476190476191,\n",
       "  0.4068783068783068])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seeds_RDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#glass数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "glass=pd.read_csv('C:/Users/liu/python works/mydata/glass.csv')\n",
    "glass_sample=glass.drop(['label'],axis=1)\n",
    "glass_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(glass_sample)))\n",
    "glass_sample['label']=glass.label\n",
    "glass_DDF=DDF(glass_sample)\n",
    "glass_RDS=RDS(glass_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.22994196213692236,\n",
       "  0.3502504601955687,\n",
       "  0.5152614693294522,\n",
       "  0.6179656597950369,\n",
       "  0.617964541090456],\n",
       " [0.21841489962105945,\n",
       "  0.2832580827219221,\n",
       "  0.36641538337498997,\n",
       "  0.46163025074578734,\n",
       "  0.5669676691123116])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glass_DDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.3128657089015378,\n",
       "  0.4696446013858823,\n",
       "  0.6143607313213735,\n",
       "  0.6641479421458776,\n",
       "  0.6678297000015545],\n",
       " [0.16508102878335887,\n",
       "  0.1999879061517375,\n",
       "  0.28439087317584455,\n",
       "  0.37590502297831174,\n",
       "  0.47871079577521564])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glass_RDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ecoli数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ecoli=pd.read_csv('C:/Users/liu/python works/mydata/ecoli.csv')\n",
    "ecoli_sample=ecoli.drop(['label'],axis=1)\n",
    "ecoli_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(ecoli_sample)))\n",
    "ecoli_sample['label']=ecoli.label\n",
    "ecoli_DDF=DDF(ecoli_sample)\n",
    "ecoli_RDS=RDS(ecoli_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5923190802503946,\n",
       "  0.7375532605862046,\n",
       "  0.7668036320900522,\n",
       "  0.7596573188617407,\n",
       "  0.6988221849245483],\n",
       " [0.09518400802944128,\n",
       "  0.1376630980260957,\n",
       "  0.26062562730010036,\n",
       "  0.3754081632653061,\n",
       "  0.5257343593174976])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecoli_DDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#image-segmentation数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9h 27min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "image=pd.read_csv('C:/Users/liu/python works/mydata/segmentation.csv')\n",
    "image_sample=image.drop(['label'],axis=1)\n",
    "image_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(image_sample)))\n",
    "image_sample['label']=image.label\n",
    "image_DDF=DDF(image_sample)\n",
    "image_RDS=RDS(image_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5379055956960178,\n",
       "  0.6992843922310165,\n",
       "  0.7836599912155218,\n",
       "  0.7737832378751784,\n",
       "  0.7557377532146783],\n",
       " [0.1012987012987013,\n",
       "  0.1522847522847523,\n",
       "  0.2672919672919673,\n",
       "  0.4057239057239057,\n",
       "  0.555122655122655])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_DDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.6375677272487286,\n",
       "  0.7743025930798952,\n",
       "  0.8575215207829532,\n",
       "  0.8678115429922583,\n",
       "  0.8438951500617764],\n",
       " [0.0836940836940837,\n",
       "  0.13492063492063494,\n",
       "  0.23915343915343917,\n",
       "  0.35170755170755175,\n",
       "  0.48417508417508426])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_RDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#statlog数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 45min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "statlog=pd.read_csv('C:/Users/liu/python works/mydata/statlog.csv',engine='python')\n",
    "statlog_sample=statlog.drop(['label'],axis=1)\n",
    "statlog_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(statlog_sample)))\n",
    "statlog_sample['label']=statlog.label  \n",
    "statlog_DDF=DDF(statlog_sample)\n",
    "statlog_RDS=RDS(statlog_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.16342568610829128,\n",
       "  0.27576134571863753,\n",
       "  0.4254894160036963,\n",
       "  0.5178071721152837,\n",
       "  0.5830636533047457],\n",
       " [0.33792771632849444,\n",
       "  0.3895423551688102,\n",
       "  0.48411125021987234,\n",
       "  0.5797300140373387,\n",
       "  0.6644346608447924])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statlog_DDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#yeast数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 14min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "yeast=pd.read_csv('C:/Users/liu/python works/mydata/yeast.csv',engine='python')\n",
    "yeast_sample=yeast.drop(['label'],axis=1)\n",
    "yeast_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(yeast_sample)))\n",
    "yeast_sample['label']=yeast.label\n",
    "yeast_DDF=DDF(yeast_sample)\n",
    "yeast_RDS=RDS(yeast_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#新甲状腺数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "thyroid=pd.read_csv('C:/Users/liu/python works/mydata/new-thyroid.csv',engine='python')\n",
    "thyroid_sample=thyroid.drop(['label'],axis=1)\n",
    "thyroid_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(thyroid_sample)))\n",
    "thyroid_sample['label']=thyroid.label\n",
    "thyroid_DDF=DDF(thyroid_sample)\n",
    "thyroid_RDS=RDS(thyroid_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5453787364687114,\n",
       "  0.6665833358206059,\n",
       "  0.7785051901383194,\n",
       "  0.7614884281943747,\n",
       "  0.7154108321260375],\n",
       " [0.10491960899524597,\n",
       "  0.15503712408525183,\n",
       "  0.25788152342289405,\n",
       "  0.37932535655146626,\n",
       "  0.511620639923081])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thyroid_DDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.563203702285512,\n",
       "  0.7250103421932208,\n",
       "  0.8364169620108516,\n",
       "  0.860105747974513,\n",
       "  0.832424677029368],\n",
       " [0.1018081299075904,\n",
       "  0.14057475562202876,\n",
       "  0.22633406335131673,\n",
       "  0.31316970247315845,\n",
       "  0.39018481918700926])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thyroid_RDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#letter数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5h 6min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "letter=pd.read_csv('C:/Users/liu/python works/mydata/letter.csv',engine='python')\n",
    "letter_sample=letter.drop(['label'],axis=1)\n",
    "letter_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(letter_sample)))\n",
    "letter_sample['label']=letter.label\n",
    "letter_DDF=DDF(letter_sample)\n",
    "letter_RDS=RDS(letter_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Contraception数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cmc=pd.read_csv('C:/Users/liu/python works/mydata/Contraception.csv',engine='python')\n",
    "cmc_sample=cmc.drop(['label'],axis=1)\n",
    "cmc_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(cmc_sample)))\n",
    "cmc_sample['label']=cmc.label\n",
    "cmc_DDF=DDF(cmc_sample)\n",
    "cmc_RDS=RDS(cmc_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#vowel数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 21min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vowel=pd.read_csv('C:/Users/liu/python works/mydata/vowel.csv',engine='python')\n",
    "vowel_sample=vowel.drop(['label'],axis=1)\n",
    "vowel_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(vowel_sample)))\n",
    "vowel_sample['label']=vowel.label\n",
    "vowel_DDF=DDF(vowel_sample)\n",
    "vowel_RDS=RDS(vowel_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.24974480071108013,\n",
       "  0.3624457298317684,\n",
       "  0.4707083777258497,\n",
       "  0.5255983777728146,\n",
       "  0.5819095633811726],\n",
       " [0.2295061728395062,\n",
       "  0.31,\n",
       "  0.4648148148148149,\n",
       "  0.6303703703703704,\n",
       "  0.7528395061728395])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vowel_DDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5332004961105561,\n",
       "  0.6162697747785237,\n",
       "  0.6709711050620416,\n",
       "  0.694183098356145,\n",
       "  0.7025162305143322],\n",
       " [0.10160493827160495,\n",
       "  0.1706172839506173,\n",
       "  0.30876543209876545,\n",
       "  0.45518518518518525,\n",
       "  0.6034567901234568])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vowel_RDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pen_base数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10h 24min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pen_base=pd.read_csv('C:/Users/liu/python works/mydata/pen_base.csv',engine='python')\n",
    "pen_sample=pen_base.drop(['label'],axis=1)\n",
    "pen_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(pen_sample)))\n",
    "pen_sample['label']=pen_base.label\n",
    "pen_DDF=DDF(pen_sample)\n",
    "pen_RDS=RDS(pen_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#landsat数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%%time\n",
    "landsat=pd.read_csv('C:/Users/liu/python works/mydata/landsat.csv',engine='python')\n",
    "landsat_sample=landsat.drop(['label'],axis=1)\n",
    "landsat_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(landsat_sample)))\n",
    "landsat_sample['label']=landsat.label\n",
    "landsat_res=DDF(landsat_sample)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#landsat_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fh_DDF=np.array(iris_DDF[0])+np.array(wine_DDF[0])+np.array(seeds_DDF[0])+np.array(glass_DDF[0])+np.array(ecoli_DDF[0])+np.array(image_DDF[0])+np.array(statlog_DDF[0])+np.array(yeast_DDF[0])+np.array(pen_DDF[0])+np.array(letter_DDF[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "Re_DDF=np.array(iris_DDF[1])+np.array(wine_DDF[1])+np.array(seeds_DDF[1])+np.array(glass_DDF[1])+np.array(ecoli_DDF[1])+np.array(image_DDF[1])+np.array(statlog_DDF[1])+np.array(yeast_DDF[1])+np.array(pen_DDF[1])+np.array(letter_DDF[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fh_RDS=np.array(iris_RDS[0])+np.array(wine_RDS[0])+np.array(seeds_RDS[0])+np.array(glass_RDS[0])+np.array(ecoli_RDS[0])+np.array(image_RDS[0])+np.array(statlog_RDS[0])+np.array(yeast_RDS[0])+np.array(pen_RDS[0])+np.array(letter_RDS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "Re_RDS=np.array(iris_RDS[1])+np.array(wine_RDS[1])+np.array(seeds_RDS[1])+np.array(glass_RDS[1])+np.array(ecoli_RDS[1])+np.array(image_RDS[1])+np.array(statlog_RDS[1])+np.array(yeast_RDS[1])+np.array(pen_RDS[1])+np.array(letter_RDS[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re_DDF=np.array([0.7,1.1,2.0,2.5,3.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re_RDS=np.array([0.7,1.3,2.2,3.2,4.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import mpl_toolkits.axisartist.axislines as axislines\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "%matplotlib\n",
    "noise_rate=[5,10,20,30,40]\n",
    "fig = plt.figure(figsize=(5,10))\n",
    "ax1 = axislines.Subplot(fig, 2,1,1)\n",
    "fig.add_subplot(ax1)\n",
    "ax1.set_xticks(noise_rate)\n",
    "ax1.axis([0,40,0,1])\n",
    "plt.plot(noise_rate,Fh_DDF/10,'r.-',label='DDF')\n",
    "plt.plot(noise_rate,Fh_RDS/10,'b*-',label='DDF-NOS')\n",
    "plt.xlabel('noise rate/% ')\n",
    "plt.ylabel('F0.5')\n",
    "#plt.title('不同噪声下算法的过滤性能')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "ax2 = axislines.Subplot(fig, 2,1,2)\n",
    "fig.add_subplot(ax2)\n",
    "ax2.set_xticks([5,10,20,30,40])\n",
    "ax2.set_yticks([0,0.2,0.4,0.6,0.8])\n",
    "ax2.axis([0,40,0,0.8])\n",
    "plt.plot(noise_rate,Re_DDF/10,'r.-',label='DDF')\n",
    "plt.plot(noise_rate,Re_RDS/10,'b*-',label='DDF-NOS')\n",
    "plt.xlabel('noise rate/%')\n",
    "plt.ylabel('Re')\n",
    "# plt.title('不同噪声下的过滤值Re',fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.tight_layout(2) #设置子图间隔\n",
    "#plt.suptitle(title)\n",
    "plt.show()\n",
    "plt.savefig(r'./对比实验结果图F0.5/10个数据集.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
