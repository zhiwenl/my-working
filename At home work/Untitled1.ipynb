{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data, depth=0, lchild=None, rchild=None):\n",
    "        self.data = data\n",
    "        self.depth = depth\n",
    "        self.lchild = lchild\n",
    "        self.rchild = rchild\n",
    "\n",
    "\n",
    "class KdTree:\n",
    "    def __init__(self):\n",
    "        self.KdTree = None\n",
    "        self.n = 0\n",
    "        self.nearest = None\n",
    "\n",
    "    def create(self, dataSet, depth=0):\n",
    "        if len(dataSet) > 0:\n",
    "            m, n = np.shape(dataSet)\n",
    "            self.n = n - 1\n",
    "            axis = depth % self.n\n",
    "            mid = int(m / 2)\n",
    "            dataSetcopy = sorted(dataSet, key=lambda x: x[axis])\n",
    "            node = Node(dataSetcopy[mid], depth)\n",
    "            if depth == 0:\n",
    "                self.KdTree = node\n",
    "            node.lchild = self.create(dataSetcopy[:mid], depth+1)\n",
    "            node.rchild = self.create(dataSetcopy[mid+1:], depth+1)\n",
    "            return node\n",
    "        return None\n",
    "\n",
    "    def preOrder(self, node):\n",
    "        if node is not None:\n",
    "            print(node.depth, node.data)\n",
    "            self.preOrder(node.lchild)\n",
    "            self.preOrder(node.rchild)\n",
    "\n",
    "    def search(self, x, count=1):\n",
    "        nearest = []\n",
    "        for i in range(count):\n",
    "            nearest.append([-1, None])\n",
    "        self.nearest = np.array(nearest)\n",
    "\n",
    "        def recurve(node):\n",
    "            if node is not None:\n",
    "                axis = node.depth % self.n\n",
    "                daxis = x[axis] - node.data[axis]\n",
    "                if daxis < 0:\n",
    "                    recurve(node.lchild)\n",
    "                else:\n",
    "                    recurve(node.rchild)\n",
    "\n",
    "                dist = sqrt(sum((p1 - p2) ** 2 for p1, p2 in zip(x, node.data)))\n",
    "                for i, d in enumerate(self.nearest):\n",
    "                    if d[0] < 0 or dist < d[0]:\n",
    "                        self.nearest = np.insert(self.nearest, i, [dist, node], axis=0)\n",
    "                        self.nearest = self.nearest[:-1]\n",
    "                        break\n",
    "\n",
    "                n = list(self.nearest[:, 0]).count(-1)\n",
    "                if self.nearest[-n-1, 0] > abs(daxis):\n",
    "                    if daxis < 0:\n",
    "                        recurve(node.rchild)\n",
    "                    else:\n",
    "                        recurve(node.lchild)\n",
    "\n",
    "        recurve(self.KdTree)\n",
    "\n",
    "        knn = self.nearest[:, 1]\n",
    "        belong = []\n",
    "        for i in knn:\n",
    "            belong.append(i.data[-1])\n",
    "        b = max(set(belong), key=belong.count)\n",
    "\n",
    "        return self.nearest, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mpl_toolkits.axisartist.axislines as axislines\n",
    "import matplotlib as mpl\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler=preprocessing.MinMaxScaler()\n",
    "mpl.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "kf=KFold(n_splits=10)\n",
    "\n",
    "\n",
    "def DDF(df):\n",
    "    data2=np.array(df.drop('label',axis=1))\n",
    "    dmax=[]\n",
    "    for m in range(0,len(data2)-1):\n",
    "        for n in range(m+1,len(data2)):\n",
    "            dmax.append(sqrt(sum((data2[m] - data2[n]) ** 2 )))\n",
    "    dmax=max(dmax)\n",
    "    #a=np.percentile(dmax,10) #密度阈值(%10分位数)\n",
    "    a=dmax*0.1\n",
    "    k=7\n",
    "    x1 = np.array([[1,1],[1,-1]])\n",
    "    x2 = np.array([1,np.floor(k / 3)/ k])\n",
    "    (Ps_,Pd_)=np.linalg.solve(x1,x2)\n",
    "    H_ = -Ps_* np.log2(Ps_)-Pd_*np.log2(Pd_) #标签混合程度阈值\n",
    "\n",
    "    data=np.array(df)\n",
    "    ACC=[]\n",
    "    for rate in [0.05,0.1,0.2,0.3,0.4]:\n",
    "        acc_temp=[]\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            train=data[train_index]\n",
    "            test=data[test_index]\n",
    "            train=pd.DataFrame(train)\n",
    "            train.columns=df.columns\n",
    "            Dn= []\n",
    "            noise_set=pd.DataFrame(columns=train.columns)\n",
    "            label_set=list(set(train.label))\n",
    "            for r in label_set:    \n",
    "                noise_set=pd.concat([noise_set,train[train.label==r].sample(frac=rate, replace=False)]) #选取噪声比例 \n",
    "            train_cut=train[~train.index.isin(noise_set.index)]\n",
    "            no=[]\n",
    "            for j in range(0,len(noise_set)):\n",
    "                no.append(np.random.choice(label_set))\n",
    "            noise_set['label2']=no\n",
    "            noise_set.reset_index(drop=True,inplace=True)\n",
    "            for l in range(0,len(noise_set)):  #随机替换标签\n",
    "                label_set=list(set(train.label))\n",
    "                if (noise_set.loc[l,'label']==noise_set.loc[l,'label2']):\n",
    "                    label_set.remove(noise_set.loc[l,'label'])\n",
    "                    noise_set.loc[l,'label2']=np.random.choice(label_set) #从剩余列表中随机选择\n",
    "            noise=noise_set.drop(['label'],axis=1)\n",
    "            noise.rename(columns={'label2':'label'}, inplace = True)\n",
    "            train= pd.concat([train_cut,noise],axis=0,ignore_index=True) #含噪声数据集\n",
    "            train_set=np.array(train)\n",
    "            kdt = KdTree()\n",
    "            kdt.create(train_set)  \n",
    "            for x in train_set:\n",
    "                near,belong = kdt.search(x[:-1], k+1)  # 设置临近点的个数\n",
    "                density = 0\n",
    "                t,hon,hen,dist_hon,dist_hen,DoD,DRL= 0,0,0,0,0,0,0\n",
    "                for n in near:\n",
    "                    if x[-1] == n[1].data[-1]:\n",
    "                        if (x != n[1].data).any():\n",
    "                            t += 1\n",
    "                            hon += 1\n",
    "                            dist_hon += n[0]\n",
    "                    else:\n",
    "                        hen += 1\n",
    "                        dist_hen += n[0]\n",
    "                    density += n[0]  #密度   \n",
    "\n",
    "                DoD = abs(dist_hen - dist_hon) #相异性差值(要加绝对值)\n",
    "                DRL = (hen - hon) / k #标签异同差比    \n",
    "                Ps = t / k\n",
    "                Pd = 1 - Ps\n",
    "                if (Ps==1)|(Ps==0):\n",
    "                    H = 0\n",
    "                else:\n",
    "                    H = -Ps* np.log2(Ps)-Pd*np.log2(Pd)\n",
    "              #  print('densit:',density)\n",
    "                if (density <= a): #高密度区域\n",
    "                    #print('高密度区')\n",
    "                    if DRL > 0:\n",
    "                        Dn.append(list(x))\n",
    "\n",
    "                else: #低密度区域\n",
    "\n",
    "                    if H >= H_: #低密度混合标签区域\n",
    "                        DoD_ = density / k\n",
    "                       # print('DoD阈值:',DoD_)\n",
    "                        if DoD >= DoD_:\n",
    "                            hon_sa,hen_sa,DRL_sa=0,0,0\n",
    "                            near2=[]\n",
    "                            for n in near:\n",
    "                                if n[0] <= a: #该点到待测点距离小于a\n",
    "                                    near2.append(n)\n",
    "                            for n2 in near2:\n",
    "                                if (x[-1]==n2[1].data[-1]):\n",
    "                                    if (x != n[1].data).any():\n",
    "                                        hon_sa +=1\n",
    "                                else:\n",
    "                                    hen_sa +=1\n",
    "                            DRL_sa = hen_sa - hon_sa\n",
    "                            if DRL_sa > 0:        \n",
    "                                Dn.append(list(x))\n",
    "                            elif DRL_sa ==0 and len(near2)<k:\n",
    "                                if x[-1] != near[len(near2)][1].data[-1]:\n",
    "                                    Dn.append(list(x)) \n",
    "                    else:#低密度单一标签区域\n",
    "                        if DRL > 0:\n",
    "                            Dn.append(list(x))\n",
    "                            \n",
    "            train_clean=train_set.tolist()\n",
    "            for i in Dn:\n",
    "                train_clean.remove(i)\n",
    "            train_clean=pd.DataFrame(train_clean)\n",
    "            train_clean.columns=df.columns\n",
    "            test=pd.DataFrame(test)\n",
    "            test.columns=df.columns\n",
    "            rf_clf=RandomForestClassifier(n_estimators=10,random_state=42)    \n",
    "            rf_clf.fit(train_clean.drop('label',axis=1),train_clean.label)\n",
    "            y_pre=rf_clf.predict(test.drop('label',axis=1))\n",
    "            acc_temp.append(accuracy_score(test.label, y_pre))   \n",
    "        ACC.append(np.mean(acc_temp))\n",
    "    return ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RDS(df):\n",
    "    data2=np.array(df.drop('label',axis=1))\n",
    "    dmax=[]\n",
    "    for m in range(0,len(data2)-1):\n",
    "        for n in range(m+1,len(data2)):\n",
    "            dmax.append(sqrt(sum((data2[m] - data2[n]) ** 2 )))\n",
    "    a=np.percentile(dmax,10) #密度阈值(%10分位数)\n",
    "    k=7\n",
    "    x1 = np.array([[1,1],[1,-1]])\n",
    "    x2 = np.array([1,np.floor(k / 3)/ k])\n",
    "    (Ps_,Pd_)=np.linalg.solve(x1,x2)\n",
    "    H_ = -Ps_* np.log2(Ps_)-Pd_*np.log2(Pd_) #标签混合程度阈值\n",
    "    data=np.array(df)\n",
    "    ACC=[]\n",
    "    for rate in [0.05,0.1,0.2,0.3,0.4]:\n",
    "        acc_temp=[]\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            train=data[train_index]\n",
    "            test=data[test_index]\n",
    "            train=pd.DataFrame(train)\n",
    "            train.columns=df.columns\n",
    "            Dn,Dn_filted= [],[]\n",
    "            noise_set=pd.DataFrame(columns=train.columns)\n",
    "            label_set=list(set(train.label))\n",
    "            for r in label_set:    \n",
    "                noise_set=pd.concat([noise_set,train[train.label==r].sample(frac=rate, replace=False)]) #选取噪声比例 \n",
    "            train_cut=train[~train.index.isin(noise_set.index)]\n",
    "            no=[]\n",
    "            for j in range(0,len(noise_set)):\n",
    "                no.append(np.random.choice(label_set))\n",
    "            noise_set['label2']=no\n",
    "            noise_set.reset_index(drop=True,inplace=True)\n",
    "            for l in range(0,len(noise_set)):  #随机替换标签\n",
    "                label_set=list(set(train.label))\n",
    "                if (noise_set.loc[l,'label']==noise_set.loc[l,'label2']):\n",
    "                    label_set.remove(noise_set.loc[l,'label'])\n",
    "                    noise_set.loc[l,'label2']=np.random.choice(label_set) #从剩余列表中随机选择\n",
    "            noise=noise_set.drop(['label'],axis=1)\n",
    "            noise.rename(columns={'label2':'label'}, inplace = True)\n",
    "            train= pd.concat([train_cut,noise],axis=0,ignore_index=True) #含噪声数据集\n",
    "            train_set=np.array(train) \n",
    "            kdt = KdTree()\n",
    "            kdt.create(train_set)\n",
    "            for x in train_set:\n",
    "                near,belong = kdt.search(x[:-1], k+1)  # 设置临近点的个数\n",
    "                density = 0\n",
    "                t,hon,hen,dist_hon,dist_hen,DoD,DRL= 0,0,0,0,0,0,0\n",
    "                for n in near:\n",
    "                    if x[-1] == n[1].data[-1]:\n",
    "                        if (x != n[1].data).any():\n",
    "                            t += 1\n",
    "                            hon += 1\n",
    "                            dist_hon += n[0]\n",
    "                    else:\n",
    "                        hen += 1\n",
    "                        dist_hen += n[0]\n",
    "                    density += n[0]  #密度   \n",
    "\n",
    "                DoD = abs(dist_hen - dist_hon) #相异性差值(要加绝对值)\n",
    "                DRL = (hen - hon) / k #标签异同差比    \n",
    "                Ps = t / k\n",
    "                Pd = 1 - Ps\n",
    "                if (Ps==1)|(Ps==0):\n",
    "                    H = 0\n",
    "                else:\n",
    "                    H = -Ps* np.log2(Ps)-Pd*np.log2(Pd)\n",
    "              #  print('densit:',density)\n",
    "                if (density <= a): #高密度区域\n",
    "                    #print('高密度区')\n",
    "                    if DRL > 0:\n",
    "                        Dn.append(list(x))\n",
    "\n",
    "                else: #低密度区域\n",
    "\n",
    "                    if H >= H_: #低密度混合标签区域\n",
    "                        DoD_ = density / k\n",
    "                       # print('DoD阈值:',DoD_)\n",
    "                        if DoD >= DoD_:\n",
    "                            train_hon,train_hen=[],[]\n",
    "                            for j in train_set:\n",
    "                                if j[-1]==x[-1]:\n",
    "                                    train_hon.append(j)\n",
    "                                else:\n",
    "                                    train_hen.append(j)\n",
    "                            kdt_hon=KdTree()\n",
    "                            kdt_hen=KdTree()\n",
    "                            kdt_hon.create(train_hon)\n",
    "                            kdt_hen.create(train_hen)\n",
    "                            near_hon,belong_hon=kdt_hon.search(x,k+1)\n",
    "                            near_hen,belong_hen=kdt_hen.search(x,k)\n",
    "                            density_hon,density_hen=0,0\n",
    "                            for hon in near_hon:\n",
    "                                density_hon += hon[0]\n",
    "                            for hen in near_hen:\n",
    "                                density_hen += hen[0]\n",
    "                            if density_hon / density_hen > 1:\n",
    "                                Dn.append(list(x))                 \n",
    "                    else:#低密度单一标签区域\n",
    "                        if DRL > 0:\n",
    "                            Dn.append(list(x))\n",
    "\n",
    "            noise_neighbor=[]                   \n",
    "            for e in Dn:  # 计算噪声分数\n",
    "                e=np.array(e)\n",
    "                near3,belong3=kdt.search(e[:-1],k+1) #k近邻\n",
    "                for n3 in near3:\n",
    "                    noise_neighbor.append(list(n3[1].data))\n",
    "            for e in Dn:\n",
    "                e=np.array(e)\n",
    "                near4,belong4=kdt.search(e[:-1],k+1)\n",
    "                t_e=noise_neighbor.count(list(e))-1 #处于其它噪声近邻的次数\n",
    "                confidence_e= 1 / np.sqrt(1+t_e*t_e)#e的置信度\n",
    "                for i in range(1,k+1):\n",
    "                    t_ei=noise_neighbor.count(list(near4[i][1].data))\n",
    "                    confidence_ei=1 / np.sqrt(1+t_ei*t_ei) \n",
    "                    if near4[i][1].data[-1]==e[-1]:\n",
    "                        different_class=-1\n",
    "                    else:\n",
    "                        different_class=1\n",
    "                    if list(near4[i][1].data) in Dn: \n",
    "                        isnoise=1\n",
    "                    else:\n",
    "                        isnoise=-1\n",
    "                    near5,belong5=kdt.search(near4[i][1].data[:-1],k+1)\n",
    "                    neighborhood_e, dis_neighbor,dis_noise=0,0,0\n",
    "                    for j in range(1,k+1):\n",
    "                        dis_neighbor += near5[j][0]\n",
    "                        if list(near5[j][1].data) in Dn:\n",
    "                            dis_noise +=near5[j][0]\n",
    "                    clean_ei= (dis_neighbor + isnoise*(dis_noise-dis_neighbor)) / (2*dis_neighbor) #纯净度\n",
    "                    neighborhood_e += (clean_ei*confidence_ei*different_class)/k \n",
    "                NS_e=confidence_e * neighborhood_e #噪声得分\n",
    "                if NS_e > 0:\n",
    "                    Dn_filted.append(list(e))\n",
    "\n",
    "                                     \n",
    "            train_clean=train_set.tolist()\n",
    "            for i in Dn_filted:\n",
    "                train_clean.remove(i)\n",
    "            train_clean=pd.DataFrame(train_clean)\n",
    "            train_clean.columns=df.columns\n",
    "            test=pd.DataFrame(test)\n",
    "            test.columns=df.columns\n",
    "            rf_clf=RandomForestClassifier(n_estimators=10,random_state=42)    \n",
    "            rf_clf.fit(train_clean.drop('label',axis=1),train_clean.label)\n",
    "            y_pre=rf_clf.predict(test.drop('label',axis=1))\n",
    "            acc_temp.append(accuracy_score(test.label,y_pre))   \n",
    "        ACC.append(np.mean(acc_temp))\n",
    "    return ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import mpl_toolkits.axisartist.axislines as axislines\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_res(res_DDF,res_RDS,title):\n",
    "    noise_rate=[5,10,20,30,40]\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    ax1 = axislines.Subplot(fig, 1,1,1)\n",
    "    fig.add_subplot(ax1)\n",
    "    ax1.set_xticks(noise_rate)\n",
    "    ax1.axis([0,40,0,1])\n",
    "    plt.plot(noise_rate,res_DDF,'r.-',label='DDF')\n",
    "    plt.plot(noise_rate,res_RDS,'b*-.',label='DDF_SCORE')\n",
    "    plt.xlabel('噪声比例/%')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris_sample= pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(iris_sample)))\n",
    "iris_sample['label'] = iris.target\n",
    "iris_DDF=DDF(iris_sample)\n",
    "iris_RDS=RDS(iris_sample)\n",
    "#plot_res(iris_DDF,iris_RDS,'(a)Iris 不同噪声下的准确率')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.datasets import load_wine\n",
    "wine= load_wine()\n",
    "wine_sample = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "wine_sample = pd.DataFrame(min_max_scaler.fit_transform(np.array(wine_sample)))\n",
    "wine_sample['label'] = wine.target\n",
    "wine_DDF=DDF(wine_sample)\n",
    "wine_RDS=RDS(wine_sample)\n",
    "#plot_res(wine_DDF,wine_RDS,'(b)Wine 不同噪声下的准确率')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seeds=pd.read_csv('C:/Users/liu/python works/mydata/seeds.csv',engine='python')\n",
    "seeds_sample=seeds.drop(['label'],axis=1)\n",
    "seeds_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(seeds_sample)))\n",
    "seeds_sample['label']=seeds.label\n",
    "seeds_DDF=DDF(seeds_sample)\n",
    "seeds_RDS=RDS(seeds_sample)\n",
    "#plot_res(seeds_DDF,seeds_RDS,'(c)Seeds 不同噪声下的准确率')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "glass=pd.read_csv('C:/Users/liu/python works/mydata/glass.csv')\n",
    "glass_sample=glass.drop(['label'],axis=1)\n",
    "glass_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(glass_sample)))\n",
    "glass_sample['label']=glass.label\n",
    "glass_DDF=DDF(glass_sample)\n",
    "glass_RDS=RDS(glass_sample)\n",
    "#plot_res(glass_DDF,glass_RDS,'(d)Glass 不同噪声下的准确率')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ecoli=pd.read_csv('C:/Users/liu/python works/mydata/ecoli.csv')\n",
    "ecoli_DDF=DDF(ecoli)\n",
    "ecoli_RDS=RDS(ecoli)\n",
    "#plot_res(ecoli_DDF,ecoli_RDS,'(e)Ecoli 不同噪声下的准确率')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "image=pd.read_csv('C:/Users/liu/python works/mydata/image.csv')\n",
    "image_sample=image.drop(['label'],axis=1)\n",
    "image_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(image_sample)))\n",
    "image_sample['label']=image.label\n",
    "image_DDF=DDF(image_sample)\n",
    "image_RDS=RDS(image_sample)\n",
    "#plot_res(image_DDF,image_RDS,'(f)Image 不同噪声下的准确率')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "newthyroid=pd.read_csv('C:/Users/liu/python works/mydata/new-thyroid.csv',engine='python')\n",
    "newthyroid_sample=newthyroid.drop(['label'],axis=1)\n",
    "newthyroid_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(newthyroid_sample)))\n",
    "newthyroid_sample['label']=newthyroid.label\n",
    "newthyroid_DDF=DDF(newthyroid_sample)\n",
    "newthyroid_RDS=RDS(newthyroid_sample)\n",
    "#plot_res(newthyroid_DDF,newthyroid_RDS,'(g)Newthyroid 不同噪声下的准确率')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 18min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cmc=pd.read_csv('C:/Users/liu/python works/mydata/Contraception.csv',engine='python')\n",
    "cmc_sample=cmc.drop(['label'],axis=1)\n",
    "cmc_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(cmc_sample)))\n",
    "cmc_sample['label']=cmc.label\n",
    "cmc_DDF=DDF(cmc_sample)\n",
    "cmc_RDS=RDS(cmc_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 30min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "statlog=pd.read_csv('C:/Users/liu/python works/mydata/statlog.csv',engine='python')\n",
    "statlog.drop_duplicates(inplace=True)\n",
    "statlog_sample=statlog[['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18']]\n",
    "statlog_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(statlog_sample)))\n",
    "statlog_sample['label']=statlog.label  \n",
    "statlog_DDF=DDF(statlog_sample)\n",
    "statlog_RDS=RDS(statlog_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 19min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "yeast=pd.read_csv('C:/Users/liu/python works/mydata/yeast.csv',engine='python')\n",
    "yeast_sample=yeast.drop(['label'],axis=1)\n",
    "yeast_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(yeast_sample)))\n",
    "yeast_sample['label']=yeast.label\n",
    "yeast_DDF=DDF(yeast_sample)\n",
    "yeast_RDS=RDS(yeast_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vowel=pd.read_csv('C:/Users/liu/python works/mydata/vowel.csv',engine='python')\n",
    "vowel_sample=vowel.drop(['label'],axis=1)\n",
    "vowel_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(vowel_sample)))\n",
    "vowel_sample['label']=vowel.label\n",
    "vowel_DDF=DDF(vowel_sample)\n",
    "vowel_RDS=RDS(vowel_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4h 8min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "letter=pd.read_csv('C:/Users/liu/python works/mydata/letter.csv',engine='python')\n",
    "letter_sample=letter.drop(['label'],axis=1)\n",
    "letter_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(letter_sample)))\n",
    "letter_sample['label']=letter.label\n",
    "letter_DDF=DDF(letter_sample)\n",
    "letter_RDS=RDS(letter_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_DDF=np.array(iris_DDF)+np.array(wine_DDF)+np.array(seeds_DDF)+np.array(glass_DDF)+np.array(ecoli_DDF)+np.array(image_DDF)+np.array(newthyroid_DDF)+np.array(cmc_DDF)+np.array(statlog_DDF)+np.array(yeast_DDF)+np.array(vowel_DDF)+np.array(letter_DDF)\n",
    "ACC_RDS=np.array(iris_RDS)+np.array(wine_RDS)+np.array(seeds_RDS)+np.array(glass_RDS)+np.array(ecoli_RDS)+np.array(image_RDS)+np.array(newthyroid_RDS)+np.array(cmc_RDS)+np.array(statlog_RDS)+np.array(yeast_RDS)+np.array(vowel_RDS)+np.array(letter_RDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_res(ACC_DDF/12,ACC_RDS/12,'不同噪声下的准确率')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
