{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data, depth=0, lchild=None, rchild=None):\n",
    "        self.data = data\n",
    "        self.depth = depth\n",
    "        self.lchild = lchild\n",
    "        self.rchild = rchild\n",
    "\n",
    "\n",
    "class KdTree:\n",
    "    def __init__(self):\n",
    "        self.KdTree = None\n",
    "        self.n = 0\n",
    "        self.nearest = None\n",
    "\n",
    "    def create(self, dataSet, depth=0):\n",
    "        if len(dataSet) > 0:\n",
    "            m, n = np.shape(dataSet)\n",
    "            self.n = n - 1\n",
    "            axis = depth % self.n\n",
    "            mid = int(m / 2)\n",
    "            dataSetcopy = sorted(dataSet, key=lambda x: x[axis])\n",
    "            node = Node(dataSetcopy[mid], depth)\n",
    "            if depth == 0:\n",
    "                self.KdTree = node\n",
    "            node.lchild = self.create(dataSetcopy[:mid], depth+1)\n",
    "            node.rchild = self.create(dataSetcopy[mid+1:], depth+1)\n",
    "            return node\n",
    "        return None\n",
    "\n",
    "    def preOrder(self, node):\n",
    "        if node is not None:\n",
    "            print(node.depth, node.data)\n",
    "            self.preOrder(node.lchild)\n",
    "            self.preOrder(node.rchild)\n",
    "\n",
    "    def search(self, x, count=1):\n",
    "        nearest = []\n",
    "        for i in range(count):\n",
    "            nearest.append([-1, None])\n",
    "        self.nearest = np.array(nearest)\n",
    "\n",
    "        def recurve(node):\n",
    "            if node is not None:\n",
    "                axis = node.depth % self.n\n",
    "                daxis = x[axis] - node.data[axis]\n",
    "                if daxis < 0:\n",
    "                    recurve(node.lchild)\n",
    "                else:\n",
    "                    recurve(node.rchild)\n",
    "\n",
    "                dist = sqrt(sum((p1 - p2) ** 2 for p1, p2 in zip(x, node.data)))\n",
    "                for i, d in enumerate(self.nearest):\n",
    "                    if d[0] < 0 or dist < d[0]:\n",
    "                        self.nearest = np.insert(self.nearest, i, [dist, node], axis=0)\n",
    "                        self.nearest = self.nearest[:-1]\n",
    "                        break\n",
    "\n",
    "                n = list(self.nearest[:, 0]).count(-1)\n",
    "                if self.nearest[-n-1, 0] > abs(daxis):\n",
    "                    if daxis < 0:\n",
    "                        recurve(node.rchild)\n",
    "                    else:\n",
    "                        recurve(node.lchild)\n",
    "\n",
    "        recurve(self.KdTree)\n",
    "\n",
    "        knn = self.nearest[:, 1]\n",
    "        belong = []\n",
    "        for i in knn:\n",
    "            belong.append(i.data[-1])\n",
    "        b = max(set(belong), key=belong.count)\n",
    "\n",
    "        return self.nearest, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise_rate: 0.05\n",
      "noise_rate: 0.1\n",
      "noise_rate: 0.2\n",
      "noise_rate: 0.3\n",
      "noise_rate: 0.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mpl_toolkits.axisartist.axislines as axislines\n",
    "import matplotlib as mpl\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler=preprocessing.MinMaxScaler()\n",
    "mpl.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(n_splits=10)\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris_sample= pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(iris_sample)))\n",
    "iris_sample['label'] = iris.target  \n",
    "df=iris_sample\n",
    "\n",
    "data2=np.array(df.drop('label',axis=1))\n",
    "dist2=[]\n",
    "for m in range(0,len(data2)-1):\n",
    "    for n in range(m+1,len(data2)):\n",
    "        dist2.append(sqrt(sum((data2[m] - data2[n]) ** 2 )))\n",
    "a = np.percentile(dist2,10) #密度阈值(%10分位数)\n",
    "k=7\n",
    "x1 = np.array([[1,1],[1,-1]])\n",
    "x2 = np.array([1,np.floor(k / 3)/ k])\n",
    "(Ps_,Pd_)=np.linalg.solve(x1,x2)\n",
    "H_ = -Ps_* np.log2(Ps_)-Pd_*np.log2(Pd_) #标签混合程度阈值\n",
    "Fh,Re=[],[]\n",
    "data=np.array(df)\n",
    "for rate in [0.05,0.1,0.2,0.3,0.4]:\n",
    "    fh_temp,re_temp=[],[]\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        train=data[train_index]\n",
    "        test=data[test_index]\n",
    "        train=pd.DataFrame(train)\n",
    "        train.columns=df.columns\n",
    "        Dn= []\n",
    "        noise_set=pd.DataFrame(columns=train.columns)\n",
    "        label_set=list(set(train.label))\n",
    "        for r in label_set:    \n",
    "            noise_set=pd.concat([noise_set,train[train.label==r].sample(frac=rate, replace=False)]) #选取噪声比例 \n",
    "        train_cut=train[~train.index.isin(noise_set.index)]\n",
    "        no=[]\n",
    "        for j in range(0,len(noise_set)):\n",
    "            no.append(np.random.choice(label_set))\n",
    "        noise_set['label2']=no\n",
    "        noise_set.reset_index(drop=True,inplace=True)\n",
    "        for l in range(0,len(noise_set)):  #随机替换标签\n",
    "            label_set=list(set(train.label))\n",
    "            if (noise_set.loc[l,'label']==noise_set.loc[l,'label2']):\n",
    "                label_set.remove(noise_set.loc[l,'label'])\n",
    "                noise_set.loc[l,'label2']=np.random.choice(label_set) #从剩余列表中随机选择\n",
    "        noise=noise_set.drop(['label'],axis=1)\n",
    "        noise.rename(columns={'label2':'label'}, inplace = True)\n",
    "        train= pd.concat([train_cut,noise],axis=0,ignore_index=True) #含噪声数据集\n",
    "        train_set=np.array(train)\n",
    "        kdt = KdTree()\n",
    "        kdt.create(train_set)  \n",
    "        for x in train_set:\n",
    "        #plt.scatter(x[0], x[1], c='red', marker='x')  # 测试点\n",
    "            near,belong = kdt.search(x[:-1], k+1)  # 设置临近点的个数\n",
    "            density = 0\n",
    "            t,hon,hen,dist_hon,dist_hen,DoD,DRL= 0,0,0,0,0,0,0\n",
    "            for n in near:\n",
    "                if x[-1] == n[1].data[-1]:\n",
    "                    if (x != n[1].data).any():\n",
    "                        t += 1\n",
    "                        hon += 1\n",
    "                        dist_hon += n[0]\n",
    "                else:\n",
    "                    hen += 1\n",
    "                    dist_hen += n[0]\n",
    "\n",
    "                density += n[0]  #密度   \n",
    "\n",
    "            DoD = abs(dist_hen - dist_hon) #相异性差值(要加绝对值)\n",
    "            DRL = (hen - hon) / k #标签异同差比    \n",
    "            Ps = t / k\n",
    "            Pd = 1 - Ps\n",
    "            if (Ps==1)|(Ps==0):\n",
    "                H = 0\n",
    "            else:\n",
    "                H = -Ps* np.log2(Ps)-Pd*np.log2(Pd)\n",
    "          #  print('densit:',density)\n",
    "            if (density <= a): #高密度区域\n",
    "                #rint('高密度区')\n",
    "                if DRL > 0:\n",
    "                    Dn.append(list(x))\n",
    "                    print('高密度区噪声')\n",
    "\n",
    "            else:\n",
    "                if (H < H_): #低密度单一标签区域\n",
    "\n",
    "                    if DRL > 0:\n",
    "                        Dn.append(list(x))\n",
    "                       # print('低密度单一标签噪声')\n",
    "                else: #低密度混合标签区  \n",
    "                  #  print('DoD',DoD)\n",
    "                    DoD_ = density / k\n",
    "                   # print('DoD阈值:',DoD_)\n",
    "                    if DoD >= DoD_:\n",
    "                        hon_sa,hen_sa,DRL_sa=0,0,0\n",
    "                        near2=[]\n",
    "                        for n in near:\n",
    "                            if n[0] <= a: #该点到待测点距离小于a\n",
    "                                near2.append(n)\n",
    "                        for n2 in near2:\n",
    "                            if (x[-1]==n2[1].data[-1]):\n",
    "                                if (x != n[1].data).any():\n",
    "                                    hon_sa +=1\n",
    "                            else:\n",
    "                                hen_sa +=1\n",
    "                        DRL_sa = hen_sa - hon_sa\n",
    "                        if DRL_sa > 0:        \n",
    "                            Dn.append(list(x))\n",
    "                           # print('混合标签区噪声')\n",
    "                        elif DRL_sa ==0 and len(near2)<k:\n",
    "                            if x[-1] != near[len(near2)][1].data[-1]:\n",
    "                                Dn.append(list(x))  \n",
    "        noise_list=noise.values.tolist()\n",
    "        TP = 0\n",
    "        for c in Dn:\n",
    "            if c in noise_list:\n",
    "                TP += 1\n",
    "        FP = len(Dn) - TP\n",
    "        FN = len(noise_list) - TP\n",
    "        precision= TP / (TP + FP)\n",
    "        recall= TP / (TP + FN)\n",
    "        fh= 1.25* (precision * recall) / (0.25*precision + recall) #F0.5值\n",
    "        re = len(Dn) / len(train_set)#移除率\n",
    "        # false_re= FP / len(data)#错误移除比率\n",
    "        fh_temp.append(fh)\n",
    "        re_temp.append(re)\n",
    "    Fh.append(np.mean(fh_temp))\n",
    "    Re.append(np.mean(re_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7203094052266216,\n",
       " 0.8593519045931263,\n",
       " 0.8972811829890107,\n",
       " 0.8815652040498231,\n",
       " 0.8249734161211031]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "glass=pd.read_csv('C:/Users/liu/python works/mydata/glass.csv')\n",
    "glass_sample=glass.drop(['label'],axis=1)\n",
    "glass_sample=pd.DataFrame(min_max_scaler.fit_transform(np.array(glass_sample)))\n",
    "glass_sample['label']=glass.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds=pd.read_csv('C:/Users/liu/python works/mydata/seeds.csv',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
